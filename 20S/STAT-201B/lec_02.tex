\lecture{2}{2020-01-22}{Markov chains}

\subsection{Kolmogorov consistency}

A crucial tool for proving existence of stochastic processes,
especially when the index set $I$ is infinite,
is \emph{Kolmogorov's consistency Theorem}.

Suppose we want to make a $\bR$-valued stochastic process
$(X_i, i \in I)$ perhaps using a large index set $I$.
Observe that for every $F \subset I$, $\lvert F \rvert < \infty$,
the law of $(X_i, i \in I)$, presuming it exists on $\prod_{I} \bR$,
induces a joint law on $(X_i, i \in F)$.
This joint law is described by the CDF $\Pr(\land_{j \in F} X_{j} \leq x_j)$
on $\bR^{\lvert F \rvert}$, which is obtained from the law on $\prod_I \bR$ by
marginalizing out $I \setminus F$.
The law on $\bR^{\lvert F \rvert}$ and the law on $\prod_I \bR$ are related:
\emph{they must be consistent}! A joint law on $(X_1, X_2)$ implies
laws for $X_1$ and $X_2$ obtained by projection (i.e. marginalization).

More generally, if $F \subset G$ then the law of $X_F \coloneqq (X_i, i \in F)$
must be the projection (marginalization) of the law of $X_G$.

\begin{definition}
  A collection of joint laws of $X_F$ on $\bR^{\lvert F \rvert}$ for each
  $F \subset I$ finite is \emph{consistent} if (as described above)
  the finite dimensional distributions agree on overlaps:
  for $F \subset J \subset I$
  \[
    \Pr_{X_J}[\land_{f \in F} \{X_f \leq x_f\} \land_{j \in J \setminus F} \{X_i \leq \infty\} ]
    = \Pr_{X_F}[\land_{f \in F} X_f \leq x_f]
  \]
\end{definition}

\begin{theorem}[Kolmogorov]
  For every consistent collection of finite dimensional distributions (FDDs),
  there exists a unique probability law on $(\bR^I, \otimes_{i \in I} \cB)$ with these
  prescribed FDDs.
\end{theorem}

\begin{remark}
  We hardly ever use it, because in nearly all interesting cases it is obvious
  how to construct $(X_i, i \in I)$ once you know the FDDs.
\end{remark}

\begin{remark}
  Unless $I$ is countable, there is a serious issue that too few sets in
  $\bR^I$ are measurable for us to care about this product construction.
\end{remark}

\begin{exercise}
  Show that every measurable set in $\bR^I$ is generated by some countable
  set of coordinates: if $B \subset \bR^I$ in product Borel $\sigma$-field,
  then $B \in \sigma(X_{i_1}, X_{i_2}, \ldots)$ for some sequence $i_1, i_2,
  \ldots$.

  As a result, with $I = [0,1]$ the set of continuous paths is not
  measurable!
\end{exercise}

\begin{proof}
  This is clearly true for $I$ countable.
  For uncountable $I$, the open sets in the product topology
  (careful; not the box topology!) on $\bR^I$ are generated
  by a basis of open cylinder sets $A$ which only have finitely
  many projections $\pi_i(A) \neq \bR$.
  Thus, the corresponding Borel $\sigma$-field is generated
  by countable operations on these sets
  and hence differ from $\bR$ on at most a countable
  number of $i \in I$.
\end{proof}

\begin{example}[Continuum of IID r.v.s]
  Suppose $\omega$ is uniformly distributed on $[0,1]$ and consider
  its binary expansion 
  \[
    \omega = \sum_{n=1}^\infty X_n(\omega) 2^{-n}
  \]
  We can construct infintely many IID uniforms on $[0,1]$ by setting
  \begin{align*}
    U_1 &= \sum_{i \geq 1} X_{2^i} 2^{-i} \\
    U_2 &= \sum_{i \geq 1} X_{3^i} 2^{-i} \\
    U_3 &= \sum_{i \geq 1} X_{5^i} 2^{-i}
  \end{align*}
  continuing with other powers of primes to maintain independence.
  Using inverse CDF, the sequence
  \[
    (F^{-1}(U_1), F^{-1}(U_2), \ldots)
  \]
  consist of IID RVs all with distribution $F$.

  Using Kolmogorov consistency theorem, we can construct a continuum
  rather than just a sequence.
  The family of FDDs given by
  \begin{align}
    \nu_{t_i,\ldots,t_n}(A_1 \times \cdots \times A_n)
    = \prod_{i=1}^n F(A_i)
  \end{align}
  for $(t_i)_{i=1}^n \subset \bR$ are consistent and clearly independent,
  hence by Kolmogorov it extends uniquely to a probability distribution
  on $\bR$ where the FDDs are IID with distribution $F$.
\end{example}

\subsection{Markov chains}
\label{sub:Markov chains}

We now move on to an important class of stochastic processes.
Let $(S, \cS)$ be the \emph{state space}, sometimes abstract.
If $S$ is countable, we will always take $\cS = 2^S$.

\begin{definition}[Markov transition function]
  Let $(S_1, \cS_1)$ and $(S_2, \cS_2)$
  be two probability spaces.
  $P$ is called a \emph{Markov transition function / kernel} if
  \begin{align*}
    P : S_1 \times \cS_2 \to [0,1]
  \end{align*}
  satisfying
  \begin{enumerate}
    \item For all $x \in S_1$, $A \mapsto P(x, A)$ is a probability measure
      on $(S_2, \cS_2)$.
    \item For fixed $A \in \cS_2$, $x \mapsto P(x, A)$ is $\cS_1$-measurable.
  \end{enumerate}
\end{definition}

Usually $\cS_2 = \sigma(\cC_2)$ for some $\pi$-system
(closed under intersection) $\cC_2$ , so we only need to check
measurability of $x \mapsto P(x, A)$ for $A \in \cC_2$. This is because
(assuming 1.) the collection
$\{A \subset S_2: x \mapsto P(x, A)~\text{measurable}\}$
is a $\lambda$-system (closed under complements and countable disjoint unions)
containing the $\pi$-system $\cC_2$
so applying Dynkin's $\pi$-$\lambda$ theorem gives the desired conclusion.

\begin{theorem}
  Let $\lambda$ be a probability distribution on $(S_1, \cS_1)$
  and $P(\cdot, \cdot)$ a transition function for $(S_1, \cS_1) \to (S_2, \cS_2)$.
  Then $\exists ! $ joint law of $(X_1, X_2)$ on
  $(S_1 \times S_2, \cS_1 \otimes \cS_2)$ such that
  \begin{align*}
    X_1 &\sim \lambda \\
    X_2 \mid X_1 = x &\sim P(x, \cdot)
  \end{align*}
\end{theorem}

Proving the above, as well as many other proofs related to Markov chains,
will require Fubini's theorem to allow us to exchange order of integration.

\begin{theorem}[Fubini]
  \label{thm:fubini}
  \[
    \bE g(X_1, X_2) = \int_{S_1} \int_{S_2} g(x_1, x_2)
    \underbrace{\Pr(X_1 \in dx_1)}_{\lambda(dx_1)}
    \underbrace{\Pr(X_2 \in dx_2 \mid X_1 = x_1)}_{P(x_1, dx_2)}
  \]
  for every product measurable $g$ which is either bounded, non-negative, or
  absolutely integrable.
\end{theorem}

\begin{remark}
  $\bE[g(X_1, X_2) \mid X_1] = \psi_g(X_1)$
  where $\psi_g(x) = \bE[g(x,X_2)]$
\end{remark}

\begin{proof}[Proof]
  We will first maake the joint law on $(S_1 \times S_2, \cS_1 \times \cS_2)$.
  Define
  \[
    I(g) = \int_{S_1} \lambda(dx_1) \int_{S_2} g(x_1, x_2) P(x_1, dx_2)
  \]
  Potential issues with $I(g)$:
  \begin{itemize}
    \item Is it defined? Try $g(x_1, x_2) = f(x_1) h(x_2)$
      for $f \in m(\cS_1)$ and $h \in m(\cS_2)$.
    \item Must check the second integral is $\cS_1$-measurable
    \item OK because take $f = \ind_A$, $h = \ind_B$.
  \end{itemize}

  $g \mapsto I(g)$ is linear, bounded, non-negative, monotone.
  What is it's measure? Take $g = \ind_C$ for $C \in \cS_1 \otimes \cS_2$
  and use monotonicity of the two integrals in $I$ along with monotone
  convergence theorem to extend to bounded/non-negative/absolutely integrable
  $g$.

  Hence, $C \mapsto I(\ind_C)$ is a probability measure on
  the product space satisfying the desired goals.
\end{proof}

\begin{remark}
  This is only abstract measure theory.
  If we were in say $\bR^2$, we could appeal to Riesz representation theorem.
  \todo{How?}
\end{remark}

\begin{note}{Flipping the transition kernel}
  There is no universal Bayes theorem for inverting laws for $(X_1)$
  and $(X_2 \mid X_1)$ into laws for $(X_2)$ and $(X_1 \mid X_2)$
  \[
    \Pr[X_2 \in A] = \int \Pr[X_1 \in dx_1] \underbrace{\Pr(X_2 \in A \mid X_1 = x)}_{P(x, A)}
  \]
  Only if we assume densities relative to a product measure, then with
  great ingenuity we can flip general Markovian $P$ to
  get a $\tilde{P}$.
\end{note}

In the discrete case, these computations reduce to linear algebra:
\begin{align}
  P &= (P(x,\{y\}), x \in S_1, y \in S_2) \\
  \lambda &= (\lambda(x), x \in S)
\end{align}
where we have
\begin{align}
  \lambda(x) &= \Pr(X_1 = x) \\
  \Pr(X_2 = y) &= \sum_x \lambda(x) P(x, y) = (\lambda P)(y) \\
  \Pr(X_1 = x \mid X_2 = y) &= \frac{\lambda(x, y) P(x, y)}{(\lambda P)(y)}
  \qquad\text{for all $y : (\lambda P)(y) > 0$}
\end{align}
More generally, let $\lambda$ be the initial distribution of $X_1$,
$P$ a transition kernel $X_1 \to X_2$,
and $f \geq 0$ nonnegative measurable. Then
\begin{align*}
  \bE f(X_2) &= \int_{S_1} \lambda(dx) (P f)(x) = \int_{S_2} (\lambda P)(dy) f(y) \\
  P f(x) &= \int_{S_2} P(x, dy) f(y) \\
  (\lambda P)(A) &= \int_{S_1} \lambda(d x) P(x, A)
\end{align*}
Notice $\lambda \mapsto \lambda P$ and $f \mapsto P f$ have excellent
and obvious properties as operators on probability measures.

\begin{example}
  If $f \geq 0$, then $P f \geq 0$.

  If $0 \leq f_n \uparrow f$, then $0 \leq P f_n \uparrow P f$.
\end{example}

If $S_1$ and $S_2$ are finite, then we are doing matrix algebra where
$P$ is a $S_1 \times S_2$ stochastic matrix and
$\lambda P$ is the action of $P$ on a row vector $\lambda$ (distributions
over states, measures more generally)
and $P f$ the action on a column vector $f$ (observables over states,
non-negative functions more generally).

Now consider $S_1 = S_2$ countable.
\begin{definition}
  Say $(X_n)$ is a Markov chain with initial distribution
  $\lambda$ and stationary transition matrix $P$ if
  \[
    \Pr[X_0 = x_0, \ldots, X_n = x_n]
    = \lambda(x_0) \prod_{i=1}^n P(x_{i-1}, x_i)
  \]
\end{definition}
