\lecture{2}{2020-01-22}{Markov chains}

\subsection{Kolmogorov consistency}

A crucial tool for proving existence of sectionchastic processes is
\emph{Kolmogorov's consistency Theorem}.
Suppose we want to make a stochastic process
\[
  (X_i, i \in I), \quad X_i \in \bR
\]
perhaps using a large index set $I$.

Observe that for every $F \subset I$, $\lvert F \rvert < \infty$,
the law of $(X_i, i \in I)$, presuming it exists on $\prod_{I} \bR$,
induces a joint law on $(X_i, i \in F)$.

$\Pr(\land_{j=1}^n X_{i_j} \leq x_i)$ is known from $\prod_I \bR$, and this
is the CDF of something on $\bR^n$. Hence, the law of $(X_i, i \in F)$ is
a probability measure on $\bR^{\lvert F \rvert}$.
The law on $\bR^{\lvert F \rvert}$ and the law on $\prod_I \bR$ are related:
\emph{they must be consistent}! A joint law on $(X_1, X_2)$ implies
laws for $X_1$ and $X_2$ obtained by projection (i.e. marginalization).

More generally, if $F \subset G$ then the law of $X_F \coloneqq (X_i, i \in F)$
must be the projection of the law of $X_G$.

\begin{definition}
  A collection of joint laws of $X_F$ on $\bR^{\lvert F \rvert}$ for each
  $F \subset I$ finite is \emph{consistent} if it satisfies the above.
\end{definition}

\begin{theorem}[Kolmogorov]
  For every consistent collection of finite dimensional distributions (FDDs),
  there exists a unique probability law on $(\bR^I, \otimes_{i \in I} \cB)$ with these
  prescribed FDDs.
\end{theorem}

\begin{remark}
  We hardly ever use it, because in nearly all interesting cases it is obvious
  how to construct $(X_i, i \in I)$ once you know the FDDs.
\end{remark}

\begin{remark}
  Unless $I$ is countable, there is a serious issue that too few sets in
  $\bR^I$ are measurable for us to care about this product construction.
\end{remark}

\begin{exercise}
  Show that every measurable set in $\bR^I$ is generated by some countable
  set of coordinates: $B \subset \bR^I$ in product $\sigma$-field, then
  $B \in (X_{i_1}, X_{i_2}, \ldots)$ for some sequence $i_1, i_2, \ldots$.

  As a result, with $I = [0,1]$ the set of continuous paths is not
  measurable!
\end{exercise}

\begin{example}[Continuum of IID r.v.s]
  Given any consistent family of FDDs, we can construct an infinite sequence of
  IIDs $(X_i)$ with any prescribed law $F$.

  $\omega \in [0,1] \to (X_1(\omega), X_2(\omega), \ldots)$.
  The binary expansion of $\omega = \sum_{n=1}^\infty X_n(\omega) 2^{-n}$.

  We can construct infintely many IID uniforms on $[0,1]$ by setting
  $U_1 = \sum_{i \geq 1} X_{2^i} 2^{-i}$, $U_2 = \sum_{i \geq 1} X_{3^i} 2^{-i}$,
  continuing with other powers of primes to maintain independence.
  Then use inverse CDF to construct arbitrary law.
\end{example}

\subsection{Markov chains}%
\label{sub:Markov chains}

Let $(S, \cS)$ be the \emph{state space}, sometimes abstract.
If $S$ is countable, we will always take $\cS = 2^S$.

Good to know what works easily for $(S, \cS)$.

Start with a general definition.

\begin{definition}[Markov transition function]
  Let $(S_1, \cS_1)$ and $(S_2, \cS_2)$
  be two probability spaces.
  $P$ is called a \emph{Markov transition function / kernel} if
  \begin{align*}
    P : S_1 \times \cS_2 \to [0,1]
  \end{align*}
  satisfying
  \begin{enumerate}
    \item For all $x \in S_1$, $A \mapsto P(x, A)$ is a probability measure
      on $(S_2, \cS_2)$.
    \item For fixed $A \in \cS_2$, $x \mapsto P(x, A)$ is $\cS_1$-measurable.
  \end{enumerate}
\end{definition}

Usually $\cS_2 = \sigma(\cC_2)$ for some $\pi$-system
$\cC_2$ (closed under intersection), so we only need to check
measurability of $x \mapsto P(x, A)$ for $A \in \cC_2$. This is because
(assuming 1.) $\{A : x \mapsto P(x, A)~\text{measurable}\}$
is a $\lambda$-system containing $\pi$-system $\cC_2$,
apply Dynkin $\pi$-$\lambda$.

\begin{theorem}
  Let $\lambda$ be a probability distribution on $(S_1, \cS_1)$
  and $P(\cdot, \cdot)$ a transition function for $(S_1, \cS_1) \to (S_2, \cS_2)$.
  Then $\exists ! $ joint law of $(X_1, X_2)$ on
  $(S_1 \times S_2, \cS_1 \otimes \cS_2)$ such that
  \begin{align*}
    X_1 &\sim \lambda \\
    X_2 \mid X_1 = x &\sim P(x, \cdot)
  \end{align*}
\end{theorem}

\begin{theorem}[Fubini]
  \label{thm:fubini}
  \[
    \bE g(X_1, X_2) = \int_{S_1} \int_{S_2} g(x_1, x_2)
    \underbrace{\Pr(X_1 \in dx_1)}_{\lambda(dx_1)}
    \underbrace{\Pr(X_2 \in dx_2 \mid X_1 = x_1)}_{P(x_1, dx_2)}
  \]
  for every product measurable $g$ which is either bounded, non-negative, or
  absolutely integrable.
\end{theorem}

\begin{exercise}
  $\bE[g(X_1, X_2) \mid X_1] = \psi_g(X_1)$
  where $\psi_g(x) = ??$
\end{exercise}

\begin{proof}[Proof]
  Key idea: make the joint law on $(S_1 \times S_2, \cS_1 \times \cS_2)$
  first. Construct measure on product space, $X_1$ and $X_2$ are the
  coordinates.

  Define
  \[
    I(g) = \int_{S_1} \lambda(dx_1) \int_{S_2} g(x_1, x_2) P(x_1, dx_2)
  \]
  Potential issues with $I(g)$:
  \begin{itemize}
    \item Is it defined? Try $g(x_1, x_2) = f(x_1) h(x_2)$
      for $f \in m(\cS_1)$ and $h \in m(\cS_2)$.
    \item Must check the second integral is $\cS_1$-measurable
    \item OK because take $f = \ind_A$, $h = \ind_B$.
  \end{itemize}

  $g \mapsto I(g)$ is linear, bounded, non-negative, monotone.
  What is it's measure? Take $g = \ind_C$ for $C \in \cS_1 \otimes \cS_2$
  and use monotonicity of the two integrals in $I$ along with monotone
  convergence theorem.

  Hence, $C \mapsto I(\ind_C)$ is a probability measure on
  the product space satisfying the desired goals.
\end{proof}

\begin{remark}
  This is only abstract measure theory.
  If we were in say $\bR^2$, we could appeal to Riesz representation theorem.
  \todo{How?}
\end{remark}

\begin{note}{Flipping the transition kernel}
  There is no universal Bayes theorem for inverting laws $(X_1)$,
  CD of $(X_2 \mid X_1)$ $\leftrightarrow$ law $(X_2)$, CD of $(X_1 \mid X_2)$
  \[
    \Pr[X_2 \in A] = \int \Pr[X_1 \in dx_1] \underbrace{\Pr(X_2 \in A \mid X_1 = x)}_{P(x, A)}
  \]
  Only if we assume densities relative to a product measure, a great
  ingenuity can gen flip general Markovian $P$ to
  get a $\tilde{P}$.

  In the discrete case, these are a matrix and vector:
  \begin{align}
    P &= (P(x,\{y\}), x \in S_1, y \in S_2) \\
    \lambda &= (\lambda(x), x \in S)
  \end{align}

  \begin{align}
    \lambda(x) &= \Pr(X_1 = x) \\
    \Pr(X_2 = y) &= \sum_x \lambda(x) P(x, y) = (\lambda P)(y) \\
    \Pr(X_1 = x \mid X_2 = y) &= \frac{\lambda(x, y) P(x, y)}{(\lambda P)(y)}
  \end{align}
  for all $y : (\lambda P)(y) > 0$.
\end{note}

More generally, let $\lambda$ be the initial distribution of $X_1$,
$P$ a transition kernel $X_1 \to X_2$,
and $f \geq 0$ nonnegative measurable.
\begin{align*}
  \bE f(X_2) &= \int_{S_1} \lambda(dx) (P f)(x) = \int_{S_2} (\lambda P)(dy) f(y) \\
  P f(x) &= \int_{S_2} P(x, dy) f(y) \\
  (\lambda P)(A) &= \int_{S_1} \lambda(d x) P(x, A)
\end{align*}
Notice $\lambda \mapsto \lambda P$ and $f \mapsto P f$ have excellent
and obvious properties as operators on probability measures.

\begin{example}
  If $f \geq 0$, then $P f \geq 0$.

  If $0 \leq f_n \uparrow f$, then $0 \leq P f_n \uparrow P f$.
\end{example}

If $S_1$ and $S_2$ are finite, then we are doing matrix algebra whree
$P$ is a $S_1 \times S_2$ stochastic matrix and
$\lambda P$ is the action of $P$ on a row vector $\lambda$ (distributions
over states, measures more generally)
and $P f$ the action on a column vector $f$ (observables over states,
non-negative functions more generally).

Now consider $S_1 = S_2$ countable.
\begin{definition}
  Say $(X_n)$ is a Markov chain with initial distribution
  $\lambda$ and stationary transition matrix $P$ if
  \[
    \Pr[X_0 = x_0, \ldots, X_n = x_n]
    = \lambda(x_0) \prod_{i=1}^n P(x_{i-1}, x_i)
  \]
\end{definition}
