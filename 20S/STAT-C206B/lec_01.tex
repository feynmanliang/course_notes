\lecture{1}{2020-01-21}{Backround material}

\subsection{Ferguson distributions / Dirichlet processes}

\begin{definition}[Gamma distribution]
  Random variable $X$ supported on $(0,\infty)$ has \emph{Gamma distribution}
  with shape $\alpha > 0$ and inverse scale / rate $\beta > 0$, written $X \sim
  \Gam(\alpha, \beta)$ if it has density
  \begin{align}
    f_X(t) &= \ind\{t \in (0,\infty)\} \frac{\beta^{\alpha} t^{\alpha - 1} e^{-\beta t}}{\Gamma(\alpha)}
  \end{align}
  where $\Gamma(t) = \int_0^\infty u^{t-1} e^{-u} du$ is the Gamma function
  defined for all $\Re t > 0$ and analytically continued to $\bC \setminus \{ n \in \bZ : n < 0 \}$
  \end{definition}

  \begin{proposition}[Gamma closed under summation]
    \label{prop:gamma-closed-sum}
    If $Y \sim \Gam(\alpha, \beta)$
    and $Z \sim \Gam(\gamma, \beta)$ are independent,
    then $Y+Z \sim \Gamma(\alpha + \gamma, \beta)$.
  \end{proposition}

  \begin{proof}
    \begin{align*}
      f_{Y+Z}(t)
      &= \int_0^t f_Y(u) f_Z(t-u) du \\
      &= \frac{1}{\Gamma(\alpha) \Gamma(\gamma)} \beta^{\alpha + \gamma} e^{-\beta t}
      \int_0^t u^{\alpha - 1} (t-u)^{\gamma - 1} du \\
      &= \frac{1}{\Gamma(\alpha) \Gamma(\gamma)} \beta^{\alpha + \gamma} e^{-\beta t}
      \int_0^1 (t v)^{\alpha - 1} (t-(t v))^{\gamma - 1} t dv \\
      &= \frac{1}{\Gamma(\alpha) \Gamma(\gamma)} \beta^{\alpha + \gamma} e^{-\beta t}
      t^{\alpha + \gamma - 1} B(\alpha, \gamma)
    \end{align*}
    where
    $B(x,y) = \int_0^1 t^{x-1} (1 - t)^{y-1} dt = \frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}$
    is the beta function
  \end{proof}

  A closely related distribution obtained from concatenating Gamma random
  variables into a vector and then normalizing the sum to $1$ is the Dirichlet
  distribution.

  \begin{definition}[Dirichlet distribution]
    Let $\valpha \in (0,\infty)^K$. Random (probability) vector
    $X$ taking values on the $K-1$-dimensional probability simplex
    $\Delta^{K-1} = \{ \vx \in [0,1]^K : \sum_i x_i = 1\}$
    has \emph{Dirichlet distribution} of order $K$ and concentration parameters
    $\valpha$, denoted $X \sim \Dir(\valpha)$, if it has density
    \[
      f_X(\vx) = \ind\{\vx \in \Delta\}
      \underbrace{%
        \frac{
          \Gamma\left(\sum_{i=1}^K \alpha_i\right)
          }{
          \prod_{i=1}^K \Gamma(\alpha_i)
      }}_{\eqqcolon B(\valpha)^{-1}} \prod_{i=1}^K x_i^{\alpha_i - 1}
    \]
  \end{definition}

  \begin{proposition}[Constructing Dirichlet from Gammas]
    \label{prop:dirichlet-from-gamma}
    Let $X_1, \ldots, X_n$ be independent $\Gam(\alpha_i, \beta)$
    distributed, $S_n = \sum_{i=1}^n X_i$.
    Then $(V_i)_i = (X_i / S_n)_i \sim \Dir(\valpha)$.
  \end{proposition}

  \begin{proof}
    $S_n \sim \Gamma(\sum_i^n \alpha_i, \beta)$ by \cref{prop:gamma-closed-sum}
    and for $\vv \in \Delta^{n-1}$, we have
    \begin{align*}
      f_V(\vv)
      &= \int_0^\infty
      f_{X}\left(s v_1, \ldots, s v_{n-1}, s v_n\right)
      f_{S_n}(s) ds \\
      &= \int_0^\infty e^{-\sum_{i=1}^{n} s v_i}
      \left( \prod_{i=1}^{n}
        \frac{ (s  v_i)^{\alpha_i - 1}}{\Gamma(\alpha_i)}
      \right)
      \frac{ s^{\sum_i^n \alpha_i - 1} e^{- s}}{\Gamma(\sum_i^n \alpha_i)} ds \\
      &= \frac{1}{\prod_{1}^n \Gamma(\alpha_i)}
      \prod_{i=1}^{n} v_i^{\alpha_i - 1}
      \int_0^\infty e^{-s \cancelto{1}{\sum_1^{n} v_i}} s^{\left(\sum_1^{n} \alpha_i\right) - 1} ds \\
      &= \frac{\Gamma(\sum_{i=1}^n \alpha_i)}{\prod_{1}^n \Gamma(\alpha_i)}
      \prod_{i=1}^{n} v_i^{\alpha_i - 1}
  \end{align*}
\end{proof}

Similar to \myref{prop:gamma-closed-sum}, where adding two Gammas yielded
another Gamma where the parameters were added, Dirichlet distributions
enjoy a similar kind of closure: ``clumping'' coordinate axes together
(described below) yields another Dirichlet distribution where the
parameters of the clumped axes are summed together.

\begin{proposition}[Dirichlet clumping property]
  Suppose $X \sim \Dir(\alpha_1, \ldots, \alpha_{n})$.
  For any $r \leq n$, let $V_i = X_i$ for $i \in [r]$
  and let $V_{r+1} = \sum_{j=r+1}^n X_j$.
  Then $V \sim \Dir(\alpha_1, \ldots, \alpha_{r}, \sum_{j=r+1}^n \alpha_j)$.
\end{proposition}

\begin{proof}
  By induction, it suffices to show this for $r = n-2$. Notice
  \begin{align*}
    f(v_1, \ldots, v_r, s)
    &= B(\valpha)^{-1} \left(\prod_{i=1}^{n-1} v_i^{\alpha_i - 1}\right)
    \int \ind\left\{
      x_{n-1} + x_n = s
    \right\} x_{n-1}^{\alpha_{n-1} - 1} x_n^{\alpha_n - 1}
    dx_{n-1} dx_n \\
    &= B(\valpha)^{-1} \left(\prod_{i=1}^{n-1} v_i^{\alpha_i - 1}\right)
    \int_0^s u^{\alpha_{n-1} - 1} (s-u)^{\alpha_n - 1} du \\
    &= B(\valpha)^{-1} \left(\prod_{i=1}^{n-1} v_i^{\alpha_i - 1}\right)
    s^{\alpha_{n-1} + \alpha_n - 1} B(\alpha_{n-1}, \alpha_n)
  \end{align*}
  Since $\frac{B(\alpha_{n-1}, \alpha_n)}{B(\valpha)} = \frac{\Gamma(\sum_1^n \alpha_i)}{\Gamma(\alpha_{n-1} + \alpha_n) \prod_1^{n-2} \Gamma(\alpha_i)}$, we are done.
\end{proof}

Iterating this result over coordinate axes other than the last $n-r$,
we see that ``clumping together'' entries in a Dirichlet
random vector yields another Dirichlet random vector with parameters also
``clumped together.'' Concretely, for any mapping $\phi : [n+1] \to [m+1]$ if
$U_j = \sum_{\phi(i) = j} V_i$ then $U$ has Dirichlet distribution with
parameters $\gamma_j = \sum_{\phi(i) = j} \alpha_i$.

Generalizing this clumping property is the motivation for \emph{Ferguson
Distributions}~\citep{ferguson1973}.

\begin{definition}[Ferguson / Dirichlet process distribution]
  Let $\mu$ be a finite positive Borel measure on complete separable metric
  space $E$.
  A random probability measure $\mu^*$ on $E$ (i.e.\ a stochastic process
  indexed by a $\sigma$-algebra on $E$) has \emph{Ferguson distribution with
  parameter $\mu$} if for every finite partition $(B_i)_{i \in [r]}$ of $E$ the
  random vector
  \[
    (\mu^*(B_i))_{i \in [r]} \sim \Dir(\mu(B_1), \ldots, \mu(B_r))
  \]
\end{definition}

\begin{lemma}[Preservation of Ferguson under pushforward]
  Let $\mu^*$ be Ferguson with parameter $\mu$
  and $\phi : E \to F$ measurable.
  Then the pushforward $\mu^* \circ \phi^{-1}$ is a random
  probability measure on $F$ that has Ferguson distribution
  with parameter $\mu \circ \phi^{-1}$.
\end{lemma}

\begin{proof}
  For $(B_i)_{i \in [r]}$ a finite partition of $F$,
  $(\phi^{-1}(B_i))_i$ is a finite partition of $E$.
  Since $\mu^*$ is Ferguson
  \[
    (\mu^*(\phi^{-1}(B_i)))_i
    \sim \Dir( (\mu(\phi^{-1}(B_i)))_i)
  \]
  Hence $\mu^* \circ \phi^{-1}$ is Ferguson with parameter
  $\mu \circ \phi^{-1}$.
\end{proof}

Next, we turn to an important class of a Ferguson distributions
arising from generalizing the P\'olya urn.

\begin{definition}[Polya sequence]
  A sequence $(X_n)_{n \in \bN}$ with values in $E$ is
  a \emph{Polya sequence with parameter $\mu$} if
  for all $B \subset E$.
  \begin{align*}
    \Pr[X_1 \in B] &= \mu(B) / \mu(E) \\
    \Pr[X_{n+1} \in B \mid X_1, \ldots, X_n] &= \mu_n(B) / \mu_n(E)
  \end{align*}
  where $\mu_n = \mu + \sum_{i=1}^n \delta_{X_i}$.
\end{definition}

\begin{remark}
  When $E$ is finite (e.g. a set of colors for the balls), $(X_n)$ represents
  the result of successive draws from an urn with initially $\mu(x)$ balls of
  color $x \in E$ and after each draw a ball of the same color as the one drawn
  is added back to give an urn with color distribution $\mu_{n+1}(x)$.
\end{remark}

\cite{blackwell1973} gives the following result connecting P\'olya sequences
and Ferguson distributions.

\begin{theorem}[Polya Urn Schemes]
  Let $(X_n)$ be a Polya sequence with parameter $\mu$. Then:
  \begin{enumerate}
    \item $m_n = \mu_n / \mu_n(E)$ converges almost surely to
      a limiting discrete measure $\mu^*$
    \item $\mu^*$ has Ferguson distribution with parameter $\mu$
    \item Given $\mu^*$, $(X_i)_{i \geq 1}$ are independent with
      distribution $\mu^*$
  \end{enumerate}
\end{theorem}

\begin{proof}
  First consider $E$ finite.
  Let $\mu^*$ and $\{X_i\}$
  be random variables whose joint distribution satisfies (2.)
  and (3.).

  Let $\pi_n$ be empirical distribution of $(X_i)_{i \in [n]}$.
  $X_i \simiid \mu^*$, so by SLLN $\pi_n \asto \mu^*$ and since
  \begin{align}
    m_n = \frac{\mu + n \pi_n}{\mu(E) + n}
  \end{align}
  (1.) follows.

  To complete the proof, we show equality in distribution of $\{X_i\}$
  with a Poly\'a-$\mu$ sequence.
  This amounts to showing
  \begin{align}
    \label{eq:polya-seq-meas}
    \Pr[A] = \prod_x \mu(x)^{[n(x)]} / \mu(E)^{[n]}
  \end{align}
  where $A = \{X_i = x_i\}_{i} \in \{0,1\}^n$ and
  $n(x) = \#\{i : x_i = x\}$,
  and the rising factorial $a^{[k]} = a (a+1) \cdots (a+k-1)$.
  By the tower rule and $\{X_i\}$ IID
  \begin{align}
    \Pr[A]
    = \bE\left[
      \Pr[A \mid \mu^*]
    \right]
    = \bE\left[
      \prod_x \mu^*(x)^{n(x)}
    \right]
  \end{align}
  Since $\mu^*$ is Ferguson, viewing $E = \sqcup_{x \in E} \{x\}$
  as a partition we have
  $(\mu^*(x))_{x \in E} \sim \Dir((\mu(x))_{x \in E})$
  so the RHS is the $(n(x))_{x \in E}$ moment of the Dirichlet distribution,
  which is equal to
  \begin{align}
    \label{eq:dirichlet-moment}
    \bE\left[
      \prod_x \mu^*(x)^{n(x)}
    \right]
    &= \frac{\Gamma(\mu(E))}{\Gamma(\mu(E) + n)}
    \prod_{x} \frac{\Gamma(\mu(x) + n(x))}{\Gamma(\mu(x))}
    = \frac{1}{\mu(E)^{[n]}} \prod_x \mu(x)^{[n(x)]}
  \end{align}
  as required by \cref{eq:polya-seq-meas}.



  General $E$ follows from approximation argument.
\end{proof}

Notice that the Dirichlet moment comparison in \cref{eq:dirichlet-moment} was
the key step relating $\mu$ to $\mu^*$.

We leave the discreteness part of (1.) as an exercise, noting that
similar to how Dirichlets can be defined as a set of independent
Gammas normalized by their sum (\myref{prop:dirichlet-from-gamma})
we would expect the Dirichlet process / Ferguson random measures
to be definable as a gamma process with independent ``increments''
divided by their sum.

\begin{exercise}
  Prove every Ferguson random measure is discrete.
  (Hint: argue using moments).
\end{exercise}


\begin{remark}
  If $(X_i)$ is a Polya sequence, then
  it is a mixture of IID sequences (each
  drawn from $\mu^*$) with mixture weights given by the Ferguson distribution
  on $\mu^*$. Hence, $(X_i)$ is exchangeable i.e.\ $(X_i) \deq (X_{\sigma(i)})$
  This is already apparent in \cref{eq:polya-seq-meas}, and more generally
  de Finetti's theorem guarantees that \emph{any} exchangeable sequence
  is a mixture of IID sequences.
\end{remark}

\subsection{Construction of Haar Measure}

For a finite group $G$, the measure $\mu(g) = \frac{1}{\# G}$ is
left and right translation invariant i.e. $\mu(gA) = \mu(A) = \mu(Ag)$
for all $A \subset G$..
As we will prove, all compact groups have unique translation invariant measure,
called the Haar measure.

\begin{example}
  Let $Z_{ij} \simiid N(0,1)$ for $i,j \in [n]$ and $X$ the Gram-Schmidt
  orthonormalization of the rows of $Z$.  By rotation invariance of $Z$, we can
  show $X U \deq U X$ for all $U \in O(n)$, so $X$ has Haar measure on the
  compact (Lie) group $O(n)$.
\end{example}


\begin{definition}
  A \emph{topological vector space} (TVS) is a vector space equipped with
  a topology such that vector space operations are jointly
  continuous.
\end{definition}

\begin{example}
  $\bR^n$ with standard topology, any Banach space.
\end{example}

\begin{definition}
  \label{def:equicontinuous}
  A family $\mathfrak{G}$ of linear transformations on TVS
  $\mathfrak{X}$ is \emph{(uniformly) equicontinuous on subset $K \subset \mathfrak{X}$}
  if for every neighborhood $V$ of the origin, there exists a neighborhood
  $U$ of the origin such that
  \begin{align*}
    \forall k_1, k_2 \in K: k_1 - k_2 \in U \implies \mathfrak{G}(k_1 - k_2) \subset V
  \end{align*}
  That is, $T(k_1 - k_2) \in V$ for all $T \in \mathfrak{G}$.

  We only need to verify at the origin because linearity of $\fG$
  and vector space structure allow us to translate the neighborhoods
  to any $p \in \fX$.
\end{definition}

\begin{remark}
  Whereas ``uniform'' is used in analysis to generalize the $U$ neighborhood of
  continuity (e.g.\ the $\delta$ in $\eps$-$\delta$ definition of continuity)
  from at a particular $x_0 \in \fX$ to $\forall x \in \fX$, ``equi'' is used
  to generalize from a single $f \in \cC(\fX)$ to a family $\fG \subset
  \cC(\fX)$.
\end{remark}

\begin{definition}[In-Class]
  A \emph{locally convex topological vector space} (LCTVS) is a TVS
  such that the topology has a base consisting of convex sets.
\end{definition}

To construct Haar measure for any compact group, we will need a fix point
theorem due to Kakutani.

\begin{theorem}[Kakutani Fix Point Theorem]
  \label{thm:kakutani}
  $K$ compact convex subset of LCTVS $\mathfrak{X}$,
  $\mathfrak{G}$ group of linear transforms equicontinuous on $K$
  and such that $\mathfrak{G}(K) \subset K$,
  then $\exists p \in K$ such that
  \begin{align}
    \mathfrak{G}(p) = \{p\}
  \end{align}
\end{theorem}

\begin{proof}
  Let $P$ be the class of all non-empty compact convex subsets of $K$ which are
  $\fG$-invariant, ordered by containment.  $K \in P$ so $P$ is not empty, and
  since any descending chain in $P$ is lower bounded by the intersection of all
  of its elements (which is also in $P$) we may apply Zorn's lemma to conclude
  there is some minimal compact convex $\fG$-invariant $K_1 \subset K$.  We are
  done if $K_1 = \{p\}$ is a singleton, so assume otherwise.  We will
  contradict minimality of $K_1$ by constructing $K_2 \subsetneq K_1$ such that
  $K_2 \in P$.

  We first exploit equicontinuity to construct a convex $\fG$-invariant
  open set $U$ which approximates $K_1 - K_1$ (i.e. 
  $(1+\eps) U \supset K_1 - K_1$ but 
  $(1 - \eps) \bar{U} \not\supset K_1 - K_1$):
  \begin{itemize}
    \item
      By assumption $K_1 - K_1$ (as a Minkowski sum) contains a point other 
      than the origin, so because $\fX$ Hausdorff there exists a neighborhood of
      the origin $V \in N(0)$ such that $\bar{V} \not\supset K_1 - K_1$.
    \item $V$ may not be convex, but since $\fX$ is a LCTVS there is
      convex $V_1$ in the local base of $0$ such that $0 \in V_1 \subset V$.
    \item $V_1$ is convex, but not $\fG$-invariant.
      Note that $\fG$ is a group so $\fG \fG A = \fG A$. To exploit this idea
      and construct a $\fG$-invariant convex open set,
      we first use equicontinuity of $\mathfrak{G}$ on $K \supset K_1$ to 
      obtain $U_1 \in N(0)$ such that 
      $\fG(U_1 \cap (K_1 - K_1)) \subset \fG(U_1) \subset V_1$.
    \item Taking the convex hull (and exploiting convexity of $V_1$),
      we have
      \[
        U_2
        \coloneqq \conv(\mathfrak{G}(U_1 \cap (K_1 - K_1))) 
        \subset \conv(V_1)
        = V_1
      \]
      $U_2$ is non-empty ($0 \in U_1 \cap (K_1 - K_1)$),
      relatively open in $K_1 - K_1$
      ($T \in \fG$ invertible maps open sets to open sets),
      and $\mathfrak{G} U_2 = U_2$ because:
      \begin{itemize}
        \item $T$ is linear so $T \conv(A) &= \conv(TA)$
        \item $T \in \mathfrak{G}$ invertible ($\mathfrak{G}$ is a group)
          so $T(A \cap B) = TA \cap TB$ for sets $A, B$.
        \item $\fG \fG A = \fG A$.
      \end{itemize}
      % Additionally, $U_2 \not\supset K_1 - K_1$ because
      % $U_2 \subset V_1 \subset V \subset \bar{V} \not\supset K_1 - K_1$.
      By continuity, $\mathfrak{G} U_2 = \overline{\mathfrak{G} U_2}$.
    \item Let $U \coloneqq \delta U_2$ where
      $\delta = \inf \{ a : a > 0, a U_2 \supset K_1 - K_1 \}$,
      by compactness of $K_1 - K_1$ we have $\delta < \infty$.
      With this definition, for any $\eps \in (0,1)$
      \[
        (1+\eps) U \supset K_1 - K_1 \not\subset (1-\eps) \overline{U}
      \]
  \end{itemize}
  Note that equicontinuity was required to bound
  $U_2 \subset V_1$.

  Next, we will exploit $(1-\eps) \bar{U} \not\subset K_1 - K_1$ to
  construct a proper subset $K_2 \subsetneq K_1$ and use
  relative openness of $U$ and compactness of $K_1$ to argue non-emptiness
  by constructing $p \in K_2$, contradicting minimality.
  \begin{itemize}
    \item For the relatively open cover $\{2^{-1} U + k\}_{k \in K_1}$ of
      $K_1$, let $\{k_i\}_{i=1}^n$ index a finite subcover and define (the
      center)
      $p = \frac{1}{n} \sum_{i=1}^n k_i$.
      Notice $p \in K_1$ (by convexity of $K_1$) and
      every $k \in K_1$
      satisfies $k \in k_i + 2^{-1} U$ for some $i \in [n]$.
      Additionally, $k \in k_j + (1 + \eps) U$ for all $j$
      (since $K_1 - K_1 \subset (1+\eps) U$) so we have
      \[
        p \in \frac{1}{n} \left(2^{-1} U + (n-1) (1+\eps) U \right) + k
      \]
      Setting $\eps = \frac{1}{4(n-1)}$ we get $p \in (1 - \frac{1}{4n}) U + k$
      for each $k \in K_1$, i.e. every point in $K_1$ is within $(1 - \frac{1}{4 n} )U$
      of the ``center'' $p$.
    \item As $p$ is within a $(1-1/4n)\bar{U}$ ball of every $k \in K_1$
      we can define the non-empty set
      \[
        K_2 = K_1 \cap \bigcap_{k \in K_1} \left(
          \left(
            1 - \frac{1}{4n}
          \right) \bar{U} + k
        \right) \supset \{p\} \neq \emptyset
      \]
      Because $(1 - \frac{1}{4n} ) \bar{U} \not\supset K_1 - K_1$, we have
      $K_2 \subsetneq K_1$ is a proper subset.
    \item To contradict minimality of $K_1$, it remains to verify
      $K_2$ satisfies the desired properties.
      $K_2$ is closed and convex because (due to how we constructed $\bar{U}$)
      it is the intersection of closed convex sets.
      Further, since $T(a \bar{U}) \subset a \bar{U}$ for $T \in
      \mathfrak{G}$, we have
      \[
        T(a\bar{U} + k)
        \subset a \bar{U} + T k
        \qquad \text{for all}~T \in \mathfrak{G}, k \in K_1
      \]
      Combined with $\fG K_1 \subset K_1$
      and $T k \in K_1$ for all $k \in K_1$, we
      have that $\fG K_2 \subset K_2$.
  \end{itemize}
\end{proof}
