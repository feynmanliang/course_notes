\lecture{1}{2020-01-21}{Backround material}

\subsection{Ferguson distributions / Dirichlet processes}

\begin{definition}[Gamma distribution]
  Random variable $X$ supported on $(0,\infty)$ has \emph{Gamma distribution}
  with shape $\alpha > 0$ and inverse scale / rate $\beta > 0$, written $X \sim
  \Gam(\alpha, \beta)$ if it has density
  \begin{align}
    f_X(t) &= \ind\{t \in (0,\infty)\} \frac{\beta^{\alpha} t^{\alpha - 1} e^{-\beta t}}{\Gamma(\alpha)}
  \end{align}
  where $\Gamma(t) = \int_0^\infty u^{t-1} e^{-u} du$ is the Gamma function
  defined for all $\Re t > 0$ and analytically continued to $\bC \setminus \{ n \in \bZ : n < 0 \}$
  \end{definition}

  \begin{proposition}[Gamma closed under summation]
    \label{prop:gamma-closed-sum}
    If $Y \sim \Gam(\alpha, \beta)$
    and $Z \sim \Gam(\gamma, \beta)$ are independent,
    then $Y+Z \sim \Gamma(\alpha + \gamma, \beta)$.
  \end{proposition}

  \begin{proof}
    \begin{align*}
      f_{Y+Z}(t)
      &= \int_0^t f_Y(u) f_Z(t-u) du \\
      &= \frac{1}{\Gamma(\alpha) \Gamma(\gamma)} \beta^{\alpha + \gamma} e^{-\beta t}
      \int_0^t u^{\alpha - 1} (t-u)^{\gamma - 1} du \\
      &= \frac{1}{\Gamma(\alpha) \Gamma(\gamma)} \beta^{\alpha + \gamma} e^{-\beta t}
      \int_0^1 (t v)^{\alpha - 1} (t-(t v))^{\gamma - 1} t dv \\
      &= \frac{1}{\Gamma(\alpha) \Gamma(\gamma)} \beta^{\alpha + \gamma} e^{-\beta t}
      t^{\alpha + \gamma - 1} B(\alpha, \gamma)
    \end{align*}
    where
    $B(x,y) = \int_0^1 t^{x-1} (1 - t)^{y-1} dt = \frac{\Gamma(x) \Gamma(y)}{\Gamma(x+y)}$
    is the beta function
  \end{proof}

  A closely related distribution obtained from concatenating Gamma random
  variables into a vector and then normalizing the sum to $1$ is the Dirichlet
  distribution.

  \begin{definition}[Dirichlet distribution]
    Let $\valpha \in (0,\infty)^K$. Random (probability) vector
    $X$ taking values on the $K-1$-dimensional probability simplex
    $\Delta^{K-1} = \{ \vx \in [0,1]^K : \sum_i x_i = 1\}$
    has \emph{Dirichlet distribution} of order $K$ and concentration parameters
    $\valpha$, denoted $X \sim \Dir(\valpha)$, if it has density
    \[
      f_X(\vx) = \ind\{\vx \in \Delta\}
      \underbrace{%
        \frac{
          \Gamma\left(\sum_{i=1}^K \alpha_i\right)
          }{
          \prod_{i=1}^K \Gamma(\alpha_i)
      }}_{\eqqcolon B(\valpha)^{-1}} \prod_{i=1}^K x_i^{\alpha_i - 1}
    \]
  \end{definition}

  \begin{proposition}[Constructing Dirichlet from Gammas]
    \label{prop:dirichlet-from-gamma}
    Let $X_1, \ldots, X_n$ be independent $\Gam(\alpha_i, \beta)$
    distributed, $S_n = \sum_{i=1}^n X_i$.
    Then $(V_i)_i = (X_i / S_n)_i \sim \Dir(\valpha)$.
  \end{proposition}

  \begin{proof}
    $S_n \sim \Gamma(\sum_i^n \alpha_i, \beta)$ by \cref{prop:gamma-closed-sum}
    and for $\vv \in \Delta^{n-1}$, we have
    \begin{align*}
      f_V(\vv)
      &= \int_0^\infty
      f_{X}\left(s v_1, \ldots, s v_{n-1}, s v_n\right)
      f_{S_n}(s) ds \\
      &= \int_0^\infty e^{-\sum_{i=1}^{n} s v_i}
      \left( \prod_{i=1}^{n}
        \frac{ (s  v_i)^{\alpha_i - 1}}{\Gamma(\alpha_i)}
      \right)
      \frac{ s^{\sum_i^n \alpha_i - 1} e^{- s}}{\Gamma(\sum_i^n \alpha_i)} ds \\
      &= \frac{1}{\prod_{1}^n \Gamma(\alpha_i)}
      \prod_{i=1}^{n} v_i^{\alpha_i - 1}
      \int_0^\infty e^{-s \cancelto{1}{\sum_1^{n} v_i}} s^{\left(\sum_1^{n} \alpha_i\right) - 1} ds \\
      &= \frac{\Gamma(\sum_{i=1}^n \alpha_i)}{\prod_{1}^n \Gamma(\alpha_i)}
      \prod_{i=1}^{n} v_i^{\alpha_i - 1}
  \end{align*}
\end{proof}

Similar to \myref{prop:gamma-closed-sum}, where adding two Gammas yielded
another Gamma where the parameters were added, Dirichlet distributions
enjoy a similar kind of closure: ``clumping'' coordinate axes together
(described below) yields another Dirichlet distribution where the
parameters of the clumped axes are summed together.

\begin{proposition}[Dirichlet clumping property]
  Suppose $X \sim \Dir(\alpha_1, \ldots, \alpha_{n})$.
  For any $r \leq n$, let $V_i = X_i$ for $i \in [r]$
  and let $V_{r+1} = \sum_{j=r+1}^n X_j$.
  Then $V \sim \Dir(\alpha_1, \ldots, \alpha_{r}, \sum_{j=r+1}^n \alpha_j)$.
\end{proposition}

\begin{proof}
  By induction, it suffices to show this for $r = n-2$. Notice
  \begin{align*}
    f(v_1, \ldots, v_r, s)
    &= B(\valpha)^{-1} \left(\prod_{i=1}^{n-1} v_i^{\alpha_i - 1}\right)
    \int \ind\left\{
      x_{n-1} + x_n = s
    \right\} x_{n-1}^{\alpha_{n-1} - 1} x_n^{\alpha_n - 1}
    dx_{n-1} dx_n \\
    &= B(\valpha)^{-1} \left(\prod_{i=1}^{n-1} v_i^{\alpha_i - 1}\right)
    \int_0^s u^{\alpha_{n-1} - 1} (s-u)^{\alpha_n - 1} du \\
    &= B(\valpha)^{-1} \left(\prod_{i=1}^{n-1} v_i^{\alpha_i - 1}\right)
    s^{\alpha_{n-1} + \alpha_n - 1} B(\alpha_{n-1}, \alpha_n)
  \end{align*}
  Since $\frac{B(\alpha_{n-1}, \alpha_n)}{B(\valpha)} = \frac{\Gamma(\sum_1^n \alpha_i)}{\Gamma(\alpha_{n-1} + \alpha_n) \prod_1^{n-2} \Gamma(\alpha_i)}$, we are done.
\end{proof}

Iterating this result over coordinate axes other than the last $n-r$,
we see that ``clumping together'' entries in a Dirichlet
random vector yields another Dirichlet random vector with parameters also
``clumped together.'' Concretely, for any mapping $\phi : [n+1] \to [m+1]$ if
$U_j = \sum_{\phi(i) = j} V_i$ then $U$ has Dirichlet distribution with
parameters $\gamma_j = \sum_{\phi(i) = j} \alpha_i$.

Generalizing this clumping property is the motivation for \emph{Ferguson
Distributions}~\citep{ferguson1973}.

\begin{definition}[Ferguson / Dirichlet process distribution]
  Let $\mu$ be a finite positive Borel measure on complete separable metric
  space $E$.
  A random probability measure $\mu^*$ on $E$ (i.e.\ a stochastic process
  indexed by a $\sigma$-algebra on $E$) has \emph{Ferguson distribution with
  parameter $\mu$} if for every finite partition $(B_i)_{i \in [r]}$ of $E$ the
  random vector
  \[
    (\mu^*(B_i))_{i \in [r]} \sim \Dir(\mu(B_1), \ldots, \mu(B_r))
  \]
\end{definition}

\begin{lemma}[Preservation of Ferguson under pushforward]
  Let $\mu^*$ be Ferguson with parameter $\mu$
  and $\phi : E \to F$ measurable.
  Then the pushforward $\mu^* \circ \phi^{-1}$ is a random
  probability measure on $F$ that has Ferguson distribution
  with parameter $\mu \circ \phi^{-1}$.
\end{lemma}

\begin{proof}
  For $(B_i)_{i \in [r]}$ a finite partition of $F$,
  $(\phi^{-1}(B_i))_i$ is a finite partition of $E$.
  Since $\mu^*$ is Ferguson
  \[
    (\mu^*(\phi^{-1}(B_i)))_i
    \sim \Dir( (\mu(\phi^{-1}(B_i)))_i)
  \]
  Hence $\mu^* \circ \phi^{-1}$ is Ferguson with parameter
  $\mu \circ \phi^{-1}$.
\end{proof}

Next, we turn to an important class of a Ferguson distributions
arising from generalizing the P\'olya urn.

\begin{definition}[Polya sequence]
  A sequence $(X_n)_{n \in \bN}$ with values in $E$ is
  a \emph{Polya sequence with parameter $\mu$} if
  for all $B \subset E$.
  \begin{align*}
    \Pr[X_1 \in B] &= \mu(B) / \mu(E) \\
    \Pr[X_{n+1} \in B \mid X_1, \ldots, X_n] &= \mu_n(B) / \mu_n(E)
  \end{align*}
  where $\mu_n = \mu + \sum_{i=1}^n \delta_{X_i}$.
\end{definition}

\begin{remark}
  When $E$ is finite (e.g. a set of colors for the balls), $(X_n)$ represents
  the result of successive draws from an urn with initially $\mu(x)$ balls of
  color $x \in E$ and after each draw a ball of the same color as the one drawn
  is added back to give an urn with color distribution $\mu_{n+1}(x)$.
\end{remark}

\cite{blackwell1973} gives the following result connecting P\'olya sequences
and Ferguson distributions.

\begin{theorem}[Polya Urn Schemes]
  Let $(X_n)$ be a Polya sequence with parameter $\mu$. Then:
  \begin{enumerate}
    \item $m_n = \mu_n / \mu_n(E)$ converges almost surely to
      a limiting discrete measure $\mu^*$
    \item $\mu^*$ has Ferguson distribution with parameter $\mu$
    \item Given $\mu^*$, $(X_i)_{i \geq 1}$ are independent with
      distribution $\mu^*$
  \end{enumerate}
\end{theorem}

\begin{proof}
  First consider $E$ finite.
  Let $\mu^*$ and $\{X_i\}$
  be random variables whose joint distribution satisfies (2.)
  and (3.).

  Let $\pi_n$ be empirical distribution of $(X_i)_{i \in [n]}$.
  $X_i \simiid \mu^*$, so by SLLN $\pi_n \asto \mu^*$ and since
  \begin{align}
    m_n = \frac{\mu + n \pi_n}{\mu(E) + n}
  \end{align}
  (1.) follows.

  To complete the proof, we show equality in distribution of $\{X_i\}$
  with a Poly\'a-$\mu$ sequence.
  This amounts to showing
  \begin{align}
    \label{eq:polya-seq-meas}
    \Pr[A] = \prod_x \mu(x)^{[n(x)]} / \mu(E)^{[n]}
  \end{align}
  where $A = \{X_i = x_i\}_{i} \in \{0,1\}^n$ and
  $n(x) = \#\{i : x_i = x\}$,
  and the rising factorial $a^{[k]} = a (a+1) \cdots (a+k-1)$.
  By the tower rule and $\{X_i\}$ IID
  \begin{align}
    \Pr[A]
    = \bE\left[
      \Pr[A \mid \mu^*]
    \right]
    = \bE\left[
      \prod_x \mu^*(x)^{n(x)}
    \right]
  \end{align}
  Since $\mu^*$ is Ferguson, viewing $E = \sqcup_{x \in E} \{x\}$
  as a partition we have
  $(\mu^*(x))_{x \in E} \sim \Dir((\mu(x))_{x \in E})$
  so the RHS is the $(n(x))_{x \in E}$ moment of the Dirichlet distribution,
  which is equal to
  \begin{align}
    \label{eq:dirichlet-moment}
    \bE\left[
      \prod_x \mu^*(x)^{n(x)}
    \right]
    &= \frac{\Gamma(\mu(E))}{\Gamma(\mu(E) + n)}
    \prod_{x} \frac{\Gamma(\mu(x) + n(x))}{\Gamma(\mu(x))}
    = \frac{1}{\mu(E)^{[n]}} \prod_x \mu(x)^{[n(x)]}
  \end{align}
  as required by \cref{eq:polya-seq-meas}.



  General $E$ follows from approximation argument.
\end{proof}

Notice that the Dirichlet moment comparison in \cref{eq:dirichlet-moment} was
the key step relating $\mu$ to $\mu^*$.

We leave the discreteness part of (1.) as an exercise, noting that
similar to how Dirichlets can be defined as a set of independent
Gammas normalized by their sum (\myref{prop:dirichlet-from-gamma})
we would expect the Dirichlet process / Ferguson random measures
to be definable as a gamma process with independent ``increments''
divided by their sum.

\begin{exercise}
  Prove every Ferguson random measure is discrete.
  (Hint: argue using moments).
\end{exercise}


\begin{remark}
  If $(X_i)$ is a Polya sequence, then
  it is a mixture of IID sequences (each
  drawn from $\mu^*$) with mixture weights given by the Ferguson distribution
  on $\mu^*$. Hence, $(X_i)$ is exchangeable i.e.\ $(X_i) \deq (X_{\sigma(i)})$
  This is already apparent in \cref{eq:polya-seq-meas}, and more generally
  de Finetti's theorem guarantees that \emph{any} exchangeable sequence
  is a mixture of IID sequences.
\end{remark}

\subsection{Construction of Haar Measure}

For a finite group $G$, the measure $\mu(g) = \frac{1}{\# G}$ is
left and right translation invariant i.e. $\mu(gA) = \mu(A) = \mu(Ag)$
for all $A \subset G$..
As we will prove, all compact groups have unique translation invariant measure,
called the Haar measure.

\begin{example}
  Let $Z_{ij} \simiid N(0,1)$ for $i,j \in [n]$ and $X$ the Gram-Schmidt
  orthonormalization of the rows of $Z$.  By rotation invariance of $Z$, we can
  show $X U \deq U X$ for all $U \in O(n)$, so $X$ has Haar measure on the
  compact (Lie) group $O(n)$.
\end{example}


\begin{definition}
  A \emph{topological vector space} (TVS) is a vector space equipped with
  a topology such that vector space operations are jointly
  continuous.
\end{definition}

\begin{example}
  $\bR^n$ with standard topology, any Banach space.
\end{example}

\begin{definition}
  \label{def:equicontinuous}
  A family $\mathfrak{G}$ of linear transformations on TVS
  $\mathfrak{X}$ is \emph{(uniformly) equicontinuous on subset $K \subset \mathfrak{X}$}
  if for every neighborhood $V$ of the origin, there exists a neighborhood
  $U$ of the origin such that
  \begin{align*}
    \forall k_1, k_2 \in K: k_1 - k_2 \in U \implies \mathfrak{G}(k_1 - k_2) \subset V
  \end{align*}
  That is, $T(k_1 - k_2) \in V$ for all $T \in \mathfrak{G}$.
\end{definition}

\begin{remark}
  Whereas ``uniform'' is used in analysis to generalize the $U$ neighborhood of
  continuity (e.g.\ the $\delta$ in $\eps$-$\delta$ definition of continuity)
  from at a particular $x_0 \in \fX$ to $\forall x \in \fX$, ``equi'' is used
  to generalize from a single $f \in \cC(\fX)$ to a family $\fG \subset
  \cC(\fX)$.
\end{remark}

\cutthmoff
\begin{definition}
  A \emph{locally convex topological vector space} (LCTVS) is a TVS
  with a local base of absolutely convex absorbing sets at the origin.
\end{definition}
\cutthmon

\begin{definition}[In-Class]
  A \emph{locally convex topological vector space} (LCTVS) is a TVS
  such that the topology has a base consisting of convex sets.
\end{definition}

To construct Haar measure for any compact group, we will need a fix point
theorem due to Kakutani.

\begin{theorem}[Kakutani Fix Point Theorem]
  \label{thm:kakutani}
  $K$ compact convex subset of LCTVS $\mathfrak{X}$,
  $\mathfrak{G}$ group of linear transforms equicontinuous on $K$
  and such that $\mathfrak{G}(K) \subset K$,
  then $\exists p \in K$ such that
  \begin{align}
    \mathfrak{G}(p) = \{p\}
  \end{align}
\end{theorem}

\begin{proof}
  By Zorn's lemma (consider inclusion chains of sets satisfying these
  properties, exploiting stability under uncountable intersection),
  there is some minimal compact convex $K_1 \subset K$ such that $K_1 \neq
  \emptyset$ and $\mathfrak{G}(K_1) \subset K_1$.

  Since we are done if $K_1 = \{p\}$ is a singleton, assume otherwise.
  We will contradict minimality of $K_1$ by constructing $K_2 \subsetneq K_1$
  such that $K_2 \neq \emptyset$ and $\fG K_2 \subset K_2$.

  We first exploit equicontinuity to construct
  a minimal convex cover $U$ of $K_1 - K_1$
  such that $T(a U) = U$ for $T \in \fG$ and $a \in \bR$
  as well as $(1 - \eps) \bar{U} \not\supset K_1 - K_1$.
  \begin{itemize}
    \item
      By assumption, $K_1 - K_1$ (Minkowski sum) contains a point other than
      the origin, so there exists a neighborhood $V \ni 0$ such that
      $\bar{V} \not\supset K_1 - K_1$.
    \item Again using $\fX$ LCTVS, for some $\lvert \alpha \rvert \leq 1$,
      there is a convex neighborhood $V_1 \ni 0$ such that $\alpha V_1
      \subset V$.
    \item By equicontinuity of $\mathfrak{G}$ on $K \supset K_1$, there is
      $U_1 \in N(0)$ such that for $k_1, k_2 \in K$ and
      $k_1 - k_2 \in U_1$ we have $\mathfrak{G}(k_1 - k_2) \subset V_1$.
    \item Let
      \[
        U_2
        \coloneqq \conv(\mathfrak{G} U_1 \cap (K_1 - K_1)) \\
        = \conv(\mathfrak{G}(U_1 \cap (K_1 - K_1))) \subset V
      \]
      $U_2$
      is relatively open in $K_1 - K_1$ and satisfies
      $\mathfrak{G} U_2 = U_2 \not\supset K_1 - K_1$
      because:
      \begin{itemize}
        \item $T \in \mathfrak{G}$ is invertible ($\mathfrak{G}$ is a group)
          hence $T$ maps open sets to open sets and $T(A \cap
          B) = TA \cap TB$ for sets $A, B$.
        \item Since $T$ is linear, for any $A$
          \begin{align}
            T \conv(A) &= \conv(TA)
          \end{align}
        \item Since $\fG$ is a group, $\fG \fG A = \fG A$.
      \end{itemize}
    \item By continuity, $\mathfrak{G} U_2 = \overline{\mathfrak{G} U_2}$.
    \item Let $\delta = \inf \{ a : a > 0, a U_2 \supset K_1 - K_1 \} \geq 1$,
      by compactness $\delta < \infty$. Let $U \coloneqq \delta U_2$.
    \item For any $\eps \in (0,1)$, note
      \[
        (1+\eps) U \supset K_1 - K_1 \not\subset (1-\eps) \overline{U}
      \]
  \end{itemize}

  Next, we use compactness of $K_1$ to get a center $p$ which is within
  $(1 - \eps) \bar{U}$ of all of $K_1$. Combined with
  $(1 - \eps) \bar{U} \not\supset K_1 - K_1$, we can define
  $K_2 \subset\neq K_1$ with the desired properties.
  \begin{itemize}
    \item For the relatively open cover $\{2^{-1} U + k\}_{k \in K_1}$ of
      $K_1$, let $\{k_i\}_{i=1}^n$ index a finite subcover and define (the
      center)
      $p = \frac{1}{n} \sum_{i=1}^n k_i$.
      Then for all $k \in K_1$, $k_i - k \in 2^{-1} U$ for some $i \in [n]$ and
      since $k_j - k \in (1 + \eps) U$ for all $j \neq i$ we have
      \[
        p \in \frac{1}{n} (2^{-1} U + (n-1) (1+\eps) U) + k
      \]
      Setting $\eps = \frac{1}{4(n-1)}$ we get $p \in (1 - \frac{1}{4n}) U + k$
      for each $k \in K_1$, i.e. every point in $K_1$ is within $(1 - \frac{1}{4 n} )U$
      of the ``center'' $p$.
    \item Let
      \[
        K_2 = K_1 \cap \bigcap_{k \in K_1} \left(
          \left(
            1 - \frac{1}{4n}
          \right) \bar{U} + k
        \right) \neq \emptyset
      \]
    \item Because $(1 - \frac{1}{4n} ) \bar{U} \not\supset K_1 - K_1$, we have
      $K_2 \subsetneq K_1$.
    \item $K_2$ is closed and convex
    \item Further, since $T(a \bar{U}) \subset a \bar{U}$ for $T \in
      \mathfrak{G}$, we have
      \[
        T(a\bar{U} + k)
        \subset a \bar{U} + T k
        \qquad \mathrm{for all}~T \in \mathfrak{G}, k \in K_1
      \]
    \item Recalling $T K_1 \subset K_1$ for $T \in \mathfrak{G}$
      and that $\mathfrak{G}$ is a group, we find that
      $T K_1 = K_1$ and $\mathfrak{G} K_2 \subset K_2$,
      contradicting minimality of $K_1$.
      a contradiction
  \end{itemize}
\end{proof}
