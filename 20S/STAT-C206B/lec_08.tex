\lecture{8}{2020-02-13}{}

Current setup
\begin{itemize}
  \item $\cP$ probability measures on $(X, \cX)$
  \item $\tilde{X} = X \times X \times \cdots$, $\tilde{\cX}$ product $\sigma$-field
  \item $\tilde{P} = \{ \tilde{\pi} = \pi \otimes \pi \otimes \cdot : \pi \in \cP \}$
  \item Cylinder cets $C(E_1^{i_1}, \ldots, E_p^{i_p}) = \prod_{i=1}^\infty F_i$ where $F_{i_r} = E_r$
    and $F_i = X$ for $i \not\in \{ i_1, \ldots, i_p\}$
  \item $\tilde{S}$ exchangeable probability measures on $\tilde{X}$
\end{itemize}
Heading towards $\tilde{P}$ equal to extreme points of $\tilde{S}$.

\begin{theorem}
  \label{thm:equality-implies-extreme}
  Let $\sigma \in \tilde{S}$ such that
  \[
    \sigma C(E_1, \ldots, E_n, E_1, \ldots, E_n)
    = [\sigma C(E_1, \ldots E_n)]^2
  \]
  for all $n \in \bN$ and sets $(E_1)^n \subset \cX$. Then $\sigma$ is
  an extreme point of $\tilde{S}$.
\end{theorem}

\begin{proof}
  By contradiction, suppose $\sigma \in \tilde{S}$ is not extreme.
  Then $\sigma = \alpha \sigma' + (1 - \alpha) \sigma''$ for $\alpha \in (0,1)$
  and $\sigma' \neq \sigma'' \in \tilde{S}$.
  Since all probability measures on $\tilde{\cX}$ are determined by values on
  cylinder sets (monotone class, Dynkin $\pi$-$\lambda$), there exists
  cylinder set $B = C(E_1,\ldots,E_n)$ such that $\sigma' B \neq \sigma'' B$.
  Let
  \[
    A = C(E_1, \ldots, E_n, E_1, \ldots, E_n)
  \]
  Then in view of \cref{thm:positive-corr-cylinders}
  \begin{align*}
    \sigma A = \alpha \sigma' A + (1 - \alpha) \sigma'' A
    \geq \alpha (\sigma' B)^2 + (1 - \alpha) (\sigma'' B)^2
  \end{align*}
  By Jensen's inequality
  \[
    [\alpha \sigma' B + (1 - \alpha) \sigma'' B]^2
    < \alpha (\sigma' B)^2 + (1 - \alpha) (\sigma'' B)^2
  \]
  Hence
  \[
    \sigma A > [\alpha \sigma' B + (1 -\alpha) \sigma'' B]^2 = (\sigma B)^2
  \]
  so that strict inequality holds, a contradiction.
\end{proof}

\begin{definition}
  For a probability measure $\pi$ on a $\sigma$-algebra
  $\Upsilon$ of subsets of a set $Y$, let $E \in \Upsilon$ be
  such that $\pi E \neq 0$. Then the set function $\pi_{vert E}$
  defined on $F \in \Upsilon$ by the relation
  \[
    \pi_{\vert E}(F) = \pi(E \cap F) / \pi E
  \]
  is a probability measure on $\Upsilon$, called the \emph{conditional probability}
  given $E$.
\end{definition}

\begin{lemma}
  \label{lem:indep-cond-prob}
  $E, F \in \Upsilon$ are independent iff $\pi_{\vert E}(F) \neq \pi_{\vert F}(E)$.
\end{lemma}

\begin{theorem}
  $\tilde{P}$ is the set of extreme points of $\tilde{S}$
\end{theorem}

\begin{proof}
  If $\tilde{\pi} \in \tilde{P}$ is an IID product measure, then
  we have equality in \cref{thm:equality-implies-extreme}
  and hence an extreme point.

  For the reverse inclusion, suppose $\sigma \in \tilde{S} \setminus \tilde{P}$.
  Since $\sigma$ is exchangeable, it cannot be a product measure otherwise
  $\sigma = \otimes_i \pi_i \implies \sigma = \otimes_i \pi$
  for some $\pi$ contradicting $\sigma \not\in \tilde{P}$.

  Thus, there must be some sets $E$, $F_1, \ldots, F_n \in \cX$ such that
  \begin{equation}
    \label{eq:witness-non-equality}
    \sigma C(E, F_1, \ldots, F_n) \neq \sigma C(E) \sigma C(F_1, \ldots, F_n)
  \end{equation}
  Introduce the shift transformation: for $A \in \tilde{X}$ let
  \[
    UA = \left\{
      a \in \tilde{X} : (a_2, a_3, \ldots) \in A
    \right\}
  \]
  i.e. $U A = X \times A$. Then $A \mapsto U A$ maps $\tilde{\cX}$ into
  (not in general surjective) $\tilde{\cX}$ and
  $\sigma U A = \sigma A$ for all $A \in \tilde{\cX}$
  by exchangeability.

  The non-equality \cref{eq:witness-non-equality} can be rephrased: there
  exits $B = C(F_1, \ldots, F_n) \in \tilde{\cX}$ and $E \in \cX$ such that
  \[
    \sigma[C(E) \cap U B] \neq \sigma C(E) \cdot \sigma B
  \]
  Hence, it is impossible that $\sigma C(E)$ or $\sigma C(E^c)$ vanish,
  otherwise both sides are equal to zero or $\sigma B$.
  Thus, we can define set functions $\sigma', \sigma''$ such
  that for $A \in \tilde{\cX}$
  \[
    \sigma' A = \sigma_{\vert C(E)} (UA),
    \quad \sigma'' A = \sigma_{\vert C(E^c)}(UA)
  \]
  $\sigma'$ and $\sigma''$ are distinct elements of $\tilde{S}$
  by \cref{lem:indep-cond-prob}, and furthermore by the
  law of total probability
  \[
    \sigma = [\sigma C(E)] \sigma' + [1 - \sigma C(E)] \sigma''
  \]
  Thus, $\sigma \in \tilde{S}$ and $\sigma \not\in \tilde{P}$ implies
  $\sigma$ is not an extreme point of $\tilde{S}$.
\end{proof}

\subsection{Hausdorff moment problem}

For $n \in \bN$, let $\Pr_{n,\theta}$ be a family of distributions
with finite expectation $\theta$ and variance $\sigma_n^2(\theta)$.
Denote expectation
\[
  \bE_{n,\theta}(u) = \int_{\bR} u(x) \Pr_{n,\theta}(dx)
\]
\begin{lemma}
  Suppose $u$ bounded continuous, $\sigma_n^2(\theta) \to 0$ for each $\theta$.
  Then $\bE_{n,\theta}(u) \to u(\theta)$ and convergence is uniform
  in every finite interval on which $\sigma_n^2(\theta) \to 0$ uniformly.
\end{lemma}
In other words, $\Pr_{n,\theta} \wto \delta_\theta$ (because $L^2$ convergence
implies convergence in probability implies convergence in distribution)
hence $\bE_{n,\theta}(u) \to u(\theta)$.

\begin{proof}
  Clearly
  \[
    \lvert \bE_{n,\theta}(u) - u(\theta) \rvert
    \leq \int_{\bR} \lvert u(x) - u(\theta)\rvert \Pr_{n,\theta}(dx)
  \]
  There exists $\delta$ depending on $\theta, \eps$ such that
  $\lvert x - \theta \rvert < \delta \implies$ the integrand
  is $< \eps$.

  Outside of this neighborhood, the integrand is less than some constant $M$
  so by Chebyshev's inequality the probability carried by the region
  $\lvert x - \theta \rvert > \delta$ is less than
  $\sigma_n^2(\theta) \delta^{-2}$.
  Thus, the right side will be $< 2 \eps$ as soon as $n$ is sufficiently large
  so that $\sigma_n^2(\theta) < \eps \delta^2 / M$.
  This bound on $n$ is independent of $|theta$ if $\sigma_n^2(\theta) \to 0$
  uniformly over this interval.
\end{proof}

\begin{example}
  \label{eg:binom-uniform-approx}
  Let $\Pr_{n,\theta}$ binomial distribution concentrated on points $k/n$
  for $k \in \{0,\ldots,n\}$, then $\sigma_n^2(\theta) = \theta (1 - \theta) n^{-1} \to 0$
  and therefore
  \[
    B_{n,u}(\theta) = \sum_{k = 0}^n u\left(\frac{k}{n} \right) \binom{n}{k} \theta^k (1 - \theta)^{n-k} \to u(\theta)
  \]
  uniformly in $\theta \in [0,1]$.
  $B_{n,u}$ is called the \emph{Bernstein polynomial of degree $n$ corresponding
  to $u$}.

  This is the proof of Weierstrass approximation theorem;
  we have a polynomial in $\theta$ converging uniformly
  to a bounded continuous function $u$.
\end{example}

Aside: weak convergence in probability theory is really weak-$\ast$ convergence,
when restricted to compact spaces.

\textbf{Hausdorff Moment Problem}: Given $(c_i)$, when can we tell that
$c_k = \int_0^1 x^k \mu(dx)$ for some probability measure $\mu$
on $[0,1]$?

\begin{definition}
  The \emph{differencing operator} $\Delta$, when applied to a countable
  sequene $(a_i)$, is defined by $\Delta a_i = a_{i+1} - a_i$.
\end{definition}

It produces a new sequence $\{\Delta a_i\}_i$, and
\[
  \Delta^2 a_i = \Delta a_{i+1} - \Delta a_i
  = a_{i+2} - 2 a_{i+1} + a_i
\]
\todo{Fill in}

we notice a reciprocity relation between $\{a_i\}$ and $\{c_i\}$.
Multiply ?? by $\binom{v}{r} c_r$ and sum over $r \in \{0,\ldots,v\}$.
\todo{Fill in }
\[
  \sum_{r=0}^v c_r \binom{v}{r} \Delta^r a_i
  = \sum_{j=0}^v a_{i+j} \binom{v}{j} (-1)^{v-j} \Delta^{v-j} c_j
\]
To approximate higher derivatives,
if we let $a_k = u(x+kh)$ for a function $u$, point $x$, and a span $h > 0$,
then we define
\[
  \Delta_h u(x) = [u(x+h) - u(x)] / h
\]
be the difference ratio (approximaiton to derivative), and more generally
\[
  \Delta_h^r u(x) = h^{-r} \sum_{j=0}^r \binom{r}{j} (-1)^{r-j} u(x+jh)
\]
In particular, $\Delta_h^0 u(x) = u(x)$.

Return to \cref{eg:binom-uniform-approx}. The LHS is aa polynomial,
called the \emph{Bernstien polynomial of degree $n$} corresponding
to the given function $u$.
Denote it by $B_{n,u}$.

\todo{Fill in}

\begin{definition}
  A sequence $\{c_k\}$ such that $(-1)^r \Delta^r c_k \geq 0$
  for $r \in \bN^*$
  is called \emph{completely monotone}.
\end{definition}

Let $\Pr$ be a distribution on $[0,1]$ and $\bE(u)$ the
integral of $u$ wrt $\Pr$.  The $k$th moment is defined by
\[
  c_k = \bE(X^k) = \int_{[0,1]} x^k \Pr(dx)
\]
Successive differences shows
\[
  (-1)^r \Delta^r c_k = \bE(\mX^k (1 - \mX)^rk)
\]
so the moment sequence $\{c_k\}$ is completely monotone.
Take $u$ an arbitrary continuous function on $[0,1]$.
Integrate \cref{eg:binom-uniform-approx}  for
the Bernstein polynomial $B_{n,u}$ wrt $\Pr$. We get
\[
  \bE B_{n,u}
  = \sum_{j=0}^n u(jh) \binom{n}{j} (-1)^{n-j} \Delta^{n-j} c_j
  = \sum_{j=0}^n u(jh) p_j^{(n)}
\]
where $h = n^{-1}$ and $p_j^{(n)} = \binom{n}{j}(-1)^{n-j} \Delta^{n-j} c_j$.
Plugging in $u=1$ shows
\[
  1 = \sum_{j=0}^n p_j^{(n)}
\]
\begin{figure}[ht]
    \centering
    \incfig{commute-over-bnu}
    \caption{commute-over-Bnu}
    \label{fig:commute-over-bnu}
\end{figure}
This means that each $n$ the $p_j^{(n)}$ define a distribution
putting weight $p_j^{(n)}$ on the point $jh = j/n$, denoted by $\Pr_n$.
\todo{Fill in}.

So far $\{c_k\}$ was the moment sequence for $\Pr$. Now
let $\{c_k\}$ be an arbitrary monotone sequence and define analogously
\[
  p_j^{(n)} = \binom{n}{j} (-1)^{n-j} \Delta^{n-j} c_j
\]
Notice this turns a completely monotone sequence $\{c_k\}$
into a probability sequence.
By definition, these are nonnegative. We will show they add to $c_0$.
By the reciprocit yformula
\[
  \sum_{j=0}^n u(jh) p_j^{(n)}
  = \sum_{r=0}^n c_r \binom{n}{r} h^r \Delta_h^r u(0)
\]
For the constant function $u=1$, the RHS reduces to $c_0$ and this
proves the assertion.

Thus, any completely monotone sequence $\{c_k\}$ subject to
the norming condition $c_0 = 1$ defines
a distribution $\{p_j^{(n)}\}$.
\todo{Fill in}

Let $u$ be a degree $N$ polynomial.
Since $h = 1/n$, $\Delta_h^r u(0) \to u^{(r)}(0)$.
Furthermore $n(n-1)\cdots (n-r+1) h^r \to 1$ and the series
$\sum_{r=0}^n c_r \binom{n}{r} h^r \Delta_h^r u(0)$
only contains terms up to $r=N$ hence at most $N+1$ terms.
As $n \to \infty$, we conclude
\[
  \bE_n(u) \to \sum_{r=0}^N \frac{c_r}{r!} u^{(r)}(0)
\]
for every degree $N$ polynomial.

In particular, when $u(x) = x^r$ we get $\bE_n(X^r) \to c_r$.
In other words, \todo{}

\begin{theorem}[Hausdorff]
  The moments $\{c_r\}$ of a probability distribution on $[0,1]$
  form a completely monotone sequence with $c_0 = 1$.
  Conversely, any arbitrarily completely monotone sequence
  with $c_0 = 1$ corresponds to a unique distributino
  on $[0,1]$.
\end{theorem}

We know that for any polynomial $u$, $\bE_n(u)$ convergese to a finite limit.
From Weierstrass approximation theorem, it follows the same is true for any
function $u$ continuous in $[0,1]$. Denote the limit $\bE_n(u)$ by $\bE(u)$.

Given $\{c_k\}$, we need to show there exists
distribution $\Pr$ such that the limit $\bE(u)$
coincides with the expectation of $u$ wrt $\Pr$.
But this is immediate from the Riesz representation theorem.

Any completely monotone sequence $\{c_k\}$ with $c_0 = 1$
given by $c_k = \int_0^1 x^k \Pr(dx)$
for suitable unique $\Pr$.
Completely monotone sequenecs with $c_0 = 1$ form a convex
set in LCTVS $\bR^\infty$ with product topology whose
extreme points are of the form $c_k = x^k$ for some $x \in [0,1]$.
Show that this is true.
